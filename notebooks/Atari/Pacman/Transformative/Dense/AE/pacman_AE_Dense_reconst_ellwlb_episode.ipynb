{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_KERAS=1\n",
      "/\n",
      "/home/azeghost/git/Generative_Models\n"
     ]
    }
   ],
   "source": [
    "%env TF_KERAS = 1\n",
    "import os\n",
    "sep_local = os.path.sep\n",
    "import sys\n",
    "# sys.path.append('..' + sep_local + '..' + sep_local +'..' + sep_local + '..' + sep_local + '..'+ sep_local + '..') # For Windows import\n",
    "os.chdir('..' + sep_local + '..' + sep_local +'..' + sep_local + '..' + sep_local + '..'+ sep_local + '..') # For Linux import\n",
    "print(sep_local)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name='atari_pacman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.from_lmdb.lmdb_image_generator import get_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /home/azeghost/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd /home/azeghost/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://omnomnom.vision.rwth-aachen.de/data/atari_v1_release/mspacman.tar.gz -P /home/azeghost/datasets/ --no-check-certificate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /home/azeghost/datasets/.mspacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xf /home/azeghost/datasets/mspacman.tar.gz -C /home/azeghost/datasets/.mspacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir = '/home/azeghost/datasets/.mspacman/atari_v1/screens/mspacman' #Linux\n",
    "# #images_dir = 'C:\\\\projects\\\\pokemon\\DS06\\\\'\n",
    "# validation_percentage = 30\n",
    "# # valid_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training.generators.file_image_generator import create_image_lists, get_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imgs_list = create_image_lists(\n",
    "#     image_dir=images_dir, \n",
    "#     validation_pct=validation_percentage, \n",
    "#     valid_imgae_formats=valid_format,\n",
    "#     verbose=0,\n",
    "#     sequenced = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(imgs_list.keys(), key=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_list['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(imgs_list['1']['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sorted(imgs_list['1']['training'], key=lambda x: int(x.split('.')[0]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(imgs_list['1']['validation'], key=lambda x: int(x.split('.')[0]))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale=3\n",
    "image_size=(53, 70, 3)\n",
    "\n",
    "batch_size = 32\n",
    "EPIS_LEN = 10\n",
    "EPIS_SHIFT = 5\n",
    "\n",
    "inputs_shape = image_size\n",
    "latents_dim = 30\n",
    "intermediate_dim = 30\n",
    "lmdb_dir = '/home/azeghost/git/lmdb_new/LMDB_Datasets/.data/Pacman_LMDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformation.lmdb_transformer import LmdbTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mLooking for images in '_training'\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mLooking for images in '_training'\u001B[0m\n",
      "  \u001B[33mWARNING \u001B[0m | \u001B[33mNo files found\u001B[0m\n",
      "  \u001B[33mWARNING \u001B[0m | \u001B[33mNo files found\u001B[0m\n",
      "/home/azeghost/git/Generative_Models/transformation/file_image_generator.py:51: UserWarning: No files found\n",
      "  warnings.warn(msg)\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mLooking for images in '_validation'\u001B[0m\n",
      "  \u001B[37mDEBUG   \u001B[0m | \u001B[37mLooking for images in '_validation'\u001B[0m\n",
      "  \u001B[33mWARNING \u001B[0m | \u001B[33mNo files found\u001B[0m\n",
      "  \u001B[33mWARNING \u001B[0m | \u001B[33mNo files found\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Initializing Iterator training Number of images 1092505\n",
      "Initializing Iterator validation Number of images 468474\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator = get_generators(\n",
    "        lmdb_dir=lmdb_dir,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='episode', \n",
    "        episode_len=EPIS_LEN, \n",
    "        episode_shift=EPIS_SHIFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator =train_generator\n",
    "testing_generator = val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 53, 70, 3)\n",
      "(32, 10, 53, 70, 3)\n"
     ]
    }
   ],
   "source": [
    "for data in train_generator:\n",
    "    print(data['xt0'].shape)\n",
    "    print(data['xt1'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store 70 batches training + 3 val to lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.from_generator(\n",
    "#     lambda: training_generator, \n",
    "#     output_types=(tf.float32, tf.float32) ,\n",
    "#     output_shapes=(tf.TensorShape((batch_size, EPIS_LEN, ) + image_size), \n",
    "#                    tf.TensorShape((batch_size, EPIS_LEN, ) + image_size)\n",
    "#                   )\n",
    "# )\n",
    "\n",
    "# test_ds = tf.data.Dataset.from_generator(\n",
    "#     lambda: testing_generator,     \n",
    "#     output_types=(tf.float32, tf.float32) ,\n",
    "#     output_shapes=(tf.TensorShape((batch_size, EPIS_LEN, ) + image_size), \n",
    "#                    tf.TensorShape((batch_size, EPIS_LEN, ) + image_size)\n",
    "#                   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FlatMapDataset' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-bca06eca8bae>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtraining_generator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'FlatMapDataset' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "training_generator.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_instance_scale=1.0\n",
    "for data in train_ds:\n",
    "    _instance_scale = float(data[0].numpy().max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_instance_scale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "if isinstance(inputs_shape, Iterable):\n",
    "    _outputs_shape = np.prod(inputs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 70, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FlatMapDataset' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-e6a58c3ada56>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_generator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'FlatMapDataset' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(training_generator)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's Layers definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdDense = lambda **kwds: tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(**kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_lays = [tdDense(units=intermediate_dim//2, activation='relu'),\n",
    "            tdDense(units=intermediate_dim//2, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=latents_dim*EPIS_LEN)]\n",
    "\n",
    "dec_lays = [tf.keras.layers.Dense(units=latents_dim*EPIS_LEN, activation='relu'),\n",
    "            tf.keras.layers.Reshape(inputs_shape),\n",
    "            tdDense(units=intermediate_dim, activation='relu'),\n",
    "            tdDense(units=_outputs_shape),\n",
    "            tf.keras.layers.Reshape(inputs_shape)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = dataset_name+'AE_Dense_reconst_ell'\n",
    "#windows\n",
    "#experiments_dir='..' + sep_local + '..' + sep_local +'..' + sep_local + '..' + sep_local + '..'+sep_local+'experiments'+sep_local + model_name\n",
    "\n",
    "#linux \n",
    "experiments_dir=os.getcwd()+ sep_local  +'experiments'+sep_local + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 842 kB/s eta 0:00:01\n",
      "\u001B[?25hCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.9.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.11.2 typeguard-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-42-c31d4a10b8a2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautoencoding_basic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransformative\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAE\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mautoencoder\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mAE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/git/Generative_Models/training/autoencoding_basic/transformative/AE.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautoencoding_basic\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautoencoders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautoencoder\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mautoencoder\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mbasicAE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mautoencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbasicAE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/Generative_Models/training/autoencoding_basic/autoencoders/autoencoder.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow_addons\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRectifiedAdam\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mevaluation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mquantitive_metrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcreate_metrics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgraphs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAE_graph\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencode_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecode_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgenerate_sample\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgraphs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mload_models\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_models\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/Generative_Models/evaluation/quantitive_metrics/metrics.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mevaluation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mquantitive_metrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstructural_similarity\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprepare_ssim_multiscale\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mevaluation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mquantitive_metrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msharp_difference\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprepare_sharpdiff\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mstatistical\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mae_losses\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mprepare_mean_absolute_error\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprepare_mean_squared_error\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcreate_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs_flat_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git/Generative_Models/statistical/ae_losses.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow_probability\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtfp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mexpected_loglikelihood_with_lower_bound\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_logits\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \"\"\"expected log-likelihood of the lower bound. For this we use a bernouilli lower bound\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "from training.autoencoding_basic.transformative.AE import autoencoder as AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_shape=image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_params = \\\n",
    "[\n",
    "    {\n",
    "        'name': 'inference', \n",
    "        'inputs_shape':inputs_shape,\n",
    "        'outputs_shape':latents_dim*EPIS_LEN,\n",
    "        'layers': enc_lays\n",
    "    }\n",
    "\n",
    "    ,\n",
    "    \n",
    "        {\n",
    "        'name': 'generative', \n",
    "        'inputs_shape':latents_dim*EPIS_LEN,\n",
    "        'outputs_shape':inputs_shape,\n",
    "        'layers':dec_lays\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "_restore = os.path.join(experiments_dir, 'var_save_dir')\n",
    "create_if_not_exist(_restore)\n",
    "absolute = abspath(_restore)\n",
    "print(\"Restore_dir\",absolute)\n",
    "absolute = abspath(experiments_dir)\n",
    "print(\"Recording_dir\",absolute)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to restore trained model, set filepath=_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ae = AE( \n",
    "    name=model_name,\n",
    "    latents_dim=latents_dim,\n",
    "    batch_size=batch_size,\n",
    "    episode_len= EPIS_LEN,\n",
    "    variables_params=variables_params, \n",
    "    filepath=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ae.compile(metrics=None)\n",
    "ae.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "\n",
    "from training.callbacks.sample_generation import SampleGeneration\n",
    "from training.callbacks.save_model import ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    min_delta=1e-12, \n",
    "    patience=12, \n",
    "    verbose=1, \n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ModelSaver(filepath=_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(experiments_dir, 'csv_dir')\n",
    "create_if_not_exist(csv_dir)\n",
    "csv_dir = os.path.join(csv_dir, model_name+'.csv')\n",
    "csv_log = tf.keras.callbacks.CSVLogger(csv_dir, append=True)\n",
    "absolute = abspath(csv_dir)\n",
    "print(\"Csv_dir\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen_dir = os.path.join(experiments_dir, 'image_gen_dir')\n",
    "create_if_not_exist(image_gen_dir)\n",
    "absolute = abspath(image_gen_dir)\n",
    "print(\"Image_gen_dir\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = SampleGeneration(latents_shape=latents_dim, filepath=image_gen_dir, gen_freq=5, save_img=True, gray_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ae.fit(\n",
    "    x=train_ds,\n",
    "    input_kw=None,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=int(1e6), \n",
    "    verbose=2,\n",
    "    callbacks=[ es, ms, csv_log, sg],\n",
    "    workers=-1,\n",
    "    use_multiprocessing=True,\n",
    "    validation_data=test_ds,\n",
    "    validation_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.inception_metrics import inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_mean, is_sigma = inception_score(ae, tolerance_threshold=1e-6, max_iteration=200)\n",
    "print(f'inception_score mean: {is_mean}, sigma: {is_sigma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frechet_inception_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.inception_metrics import frechet_inception_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_score = frechet_inception_distance(ae, training_generator, tolerance_threshold=1e-6, max_iteration=10, batch_size=32)\n",
    "print(f'frechet inception distance: {fis_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptual_path_length_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.perceptual_path_length import perceptual_path_length_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_mean_score = perceptual_path_length_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=200, batch_size=32)\n",
    "print(f'perceptual path length score: {ppl_mean_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.precision_recall import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_precision_score = precision_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=200)\n",
    "print(f'precision score: {_precision_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.generativity_metrics.precision_recall import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recall_score = recall_score(ae, training_generator, tolerance_threshold=1e-6, max_iteration=200)\n",
    "print(f'recall score: {_recall_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import reconstruct_from_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'reconstruct_training_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "reconstruct_from_a_batch(ae, training_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'reconstruct_testing_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "reconstruct_from_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import generate_images_like_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'generate_training_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_like_a_batch(ae, training_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'generate_testing_images_like_a_batch_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_like_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import generate_images_randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'random_synthetic_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "generate_images_randomly(ae, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import interpolate_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'interpolate_dir')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "interpolate_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked inputs outputs and predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.generators.image_generation_testing import predict_from_a_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "save_dir = os.path.join(experiments_dir, 'predictions')\n",
    "create_if_not_exist(save_dir)\n",
    "\n",
    "predict_from_a_batch(ae, testing_generator, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}