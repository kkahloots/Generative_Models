{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "sep_local = os.path.sep\n",
    "sep_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_KERAS=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_KERAS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('..' + sep_local + '..' + sep_local + '..') # For Windows import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..' + sep_local + '..' + sep_local + '..') # For Linux import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='dsprites',\n",
      "    version=0.1.0,\n",
      "    description='dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth\n",
      "independent latent factors. These factors are *color*, *shape*, *scale*,\n",
      "*rotation*, *x* and *y* positions of a sprite.\n",
      "\n",
      "All possible combinations of these latents are present exactly once,\n",
      "generating N = 737280 total images.\n",
      "\n",
      "### Latent factor values\n",
      "\n",
      "*   Color: white\n",
      "*   Shape: square, ellipse, heart\n",
      "*   Scale: 6 values linearly spaced in [0.5, 1]\n",
      "*   Orientation: 40 values in [0, 2 pi]\n",
      "*   Position X: 32 values in [0, 1]\n",
      "*   Position Y: 32 values in [0, 1]\n",
      "\n",
      "We varied one latent at a time (starting from Position Y, then Position X, etc),\n",
      "and sequentially stored the images in fixed order.\n",
      "Hence the order along the first dimension is fixed and allows you to map back to\n",
      "the value of the latents corresponding to that image.\n",
      "\n",
      "We chose the latents values deliberately to have the smallest step changes\n",
      "while ensuring that all pixel outputs were different. No noise was added.\n",
      "',\n",
      "    homepage='https://github.com/deepmind/dsprites-dataset',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(64, 64, 1), dtype=tf.uint8),\n",
      "        'label_orientation': ClassLabel(shape=(), dtype=tf.int64, num_classes=40),\n",
      "        'label_scale': ClassLabel(shape=(), dtype=tf.int64, num_classes=6),\n",
      "        'label_shape': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
      "        'label_x_position': ClassLabel(shape=(), dtype=tf.int64, num_classes=32),\n",
      "        'label_y_position': ClassLabel(shape=(), dtype=tf.int64, num_classes=32),\n",
      "        'value_orientation': Tensor(shape=[], dtype=tf.float32),\n",
      "        'value_scale': Tensor(shape=[], dtype=tf.float32),\n",
      "        'value_shape': Tensor(shape=[], dtype=tf.float32),\n",
      "        'value_x_position': Tensor(shape=[], dtype=tf.float32),\n",
      "        'value_y_position': Tensor(shape=[], dtype=tf.float32),\n",
      "    }),\n",
      "    total_num_examples=737280,\n",
      "    splits={\n",
      "        'train': 737280,\n",
      "    },\n",
      "    supervised_keys=None,\n",
      "    citation=\"\"\"@misc{dsprites17,\n",
      "    author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},\n",
      "    title = {dSprites: Disentanglement testing Sprites dataset},\n",
      "    howpublished= {https://github.com/deepmind/dsprites-dataset/},\n",
      "    year = \"2017\",\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'dsprites'\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "info = tfds.builder(dataset_name).info\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dim = 20\n",
    "inputs_shape=(64, 64, 1) # image shape\n",
    "batch_size = 100\n",
    "latent_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 600\n",
    "TEST_BUF = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "# Construct a tf.data.Dataset\n",
    "train_ds = tfds.load(name=dataset_name, split=tfds.Split.TRAIN).shuffle(TRAIN_BUF).batch(batch_size)\n",
    "try:\n",
    "    test_ds = tfds.load(name=dataset_name, split=tfds.Split.TEST).shuffle(TEST_BUF).batch(batch_size)\n",
    "except:\n",
    "    test_ds = tfds.load(name=dataset_name, split=tfds.Split.TRAIN).shuffle(TEST_BUF).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_instance_scale=1.0\n",
    "for data in train_ds:\n",
    "    _instance_scale = float(data['image'][0].numpy().max())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_lays2 = [\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # No activation\n",
    "    tf.keras.layers.Dense(latent_dim, dtype='float32')\n",
    "]\n",
    "\n",
    "dec_lays2 = [\n",
    "    tf.keras.layers.Dense(units=16*16*32, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Reshape(target_shape=(16, 16, 32)),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'),\n",
    "    \n",
    "    # No activation\n",
    "    tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", dtype='float32')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_mean_lays = [tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=intermediate_dim, activation='relu')]\n",
    "\n",
    "enc_var_lays = [tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=intermediate_dim, activation='relu')]\n",
    "\n",
    "dec_lays = [tf.keras.layers.Dense(units=2*intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=2*intermediate_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(units=2*intermediate_dim, activation='relu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.data_and_files.file_utils import make_random_string\n",
    "#from time import gmtime, strftime\n",
    "\n",
    "#model_name = 'AE_' + make_random_string(5) + strftime(\"%a_%d_%b_%Y_%H_%M\", gmtime())\n",
    "#print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = dataset_name + 'Dense' +'AE'\n",
    "#recoding_dir='..'+sep_local+'..'+sep_local+'..'+sep_local+'recording'+sep_local + model_name\n",
    "os.getcwd()\n",
    "recoding_dir=os.getcwd()+ sep_local  +'recording'+sep_local + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording_dir /home/azeghost/git/new_GAN_models/Generative_Models/recording/dspritesDenseAE\n",
      "Current working dir /home/azeghost/git/new_GAN_models/Generative_Models\n"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "absolute = abspath(recoding_dir)\n",
    "print(\"Recording_dir\",absolute)\n",
    "print(\"Current working dir\",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.autoencoding_basic.autoencoders.autoencoder import autoencoder as AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_params = \\\n",
    "[\n",
    "    {\n",
    "        'name': 'inference', \n",
    "        'inputs_shape':inputs_shape,\n",
    "        'outputs_shape':latent_dim,\n",
    "        'layers':enc_lays2 #enc_mean_lays\n",
    "    }\n",
    "    ,\n",
    "    \n",
    "        {\n",
    "        'name': 'generative', \n",
    "        'inputs_shape':latent_dim,\n",
    "        'outputs_shape':inputs_shape,\n",
    "        'layers':dec_lays2 #dec_lays\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore_dir /home/azeghost/git/new_GAN_models/Generative_Models/recording/dspritesDenseAE/var_save_dir\n"
     ]
    }
   ],
   "source": [
    "from utils.data_and_files.file_utils import create_if_not_exist\n",
    "_restore = os.path.join(recoding_dir, 'var_save_dir')\n",
    "create_if_not_exist(_restore)\n",
    "absolute = abspath(_restore)\n",
    "print(\"Restore_dir\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[33mWARNING \u001b[0m | \u001b[33mNone\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inference\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inference_inputs (InputLayer [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 31, 31, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 86406     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "activity_regularization (Act (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "inference_outputs (Activatio (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 105,246\n",
      "Trainable params: 105,234\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[33mWARNING \u001b[0m | \u001b[33mNone\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generative\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generative_inputs (InputLaye [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8192)              57344     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 1)         289       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 1)         4         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "activity_regularization_1 (A (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "generative_outputs (Activati (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 94,597\n",
      "Trainable params: 94,595\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ae = AE( \n",
    "    name=model_name,\n",
    "    inputs_shape=inputs_shape,\n",
    "    outputs_shape=inputs_shape,\n",
    "    latent_dim=latent_dim,\n",
    "    batch_size=batch_size,\n",
    "    variables_params=variables_params, \n",
    "    filepath=None #to restore trained model, set filepath=_restore\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dspritesDenseAE\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inference_inputs (InputLayer [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "inference (Model)            (None, 6)                 105246    \n",
      "_________________________________________________________________\n",
      "generative (Model)           (None, 64, 64, 1)         94597     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_x_logits (Tensor [(None, 64, 64, 1)]       0         \n",
      "=================================================================\n",
      "Total params: 199,843\n",
      "Trainable params: 199,829\n",
      "Non-trainable params: 14\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#ae.compile(metrics=None)\n",
    "ae.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "from training.callbacks.progress_bar import NotebookPrograssBar\n",
    "from training.callbacks.sample_generation import SampleGeneration\n",
    "from training.callbacks.save_model import ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "progbar = NotebookPrograssBar(leave_outer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    min_delta=1e-12, \n",
    "    patience=5, \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms = ModelSaver(filepath=_restore,save_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv_dir /home/azeghost/git/new_GAN_models/Generative_Models/recording/dspritesDenseAE/csv_dir/dspritesDenseAE.csv\n"
     ]
    }
   ],
   "source": [
    "csv_dir = os.path.join(recoding_dir, 'csv_dir')\n",
    "create_if_not_exist(csv_dir)\n",
    "csv_dir = os.path.join(csv_dir, model_name+'.csv')\n",
    "csv_log = tf.keras.callbacks.CSVLogger(csv_dir, append=True)\n",
    "absolute = abspath(csv_dir)\n",
    "print(\"Csv_dir\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_gen_dir /home/azeghost/git/new_GAN_models/Generative_Models/recording/dspritesDenseAE/image_gen_dir\n"
     ]
    }
   ],
   "source": [
    "image_gen_dir = os.path.join(recoding_dir, 'image_gen_dir')\n",
    "create_if_not_exist(image_gen_dir)\n",
    "absolute = abspath(image_gen_dir)\n",
    "print(\"Image_gen_dir\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sg = SampleGeneration(latent_shape=6, filepath=image_gen_dir, gen_freq=5, save_img=True, gray_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dSprites dataset.\r\n",
      "Downloading dSprites completed!\r\n"
     ]
    }
   ],
   "source": [
    "#DATA_DOWN_PATH = '..'+sep_local+'..'+sep_local+'..'+sep_local+'data'\n",
    "DATA_DOWN_PATH = os.getcwd() + sep_local+'data'\n",
    "Script_dir = os.getcwd() + sep_local+'data'+sep_local+'download_gt_data.sh'\n",
    "# Script call to download \"dsprites_full\" dataset_name \n",
    "!/bin/bash $Script_dir -f $DATA_DOWN_PATH -d $dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH /home/azeghost/git/new_GAN_models/Generative_Models/data/.gt_datasets\n"
     ]
    }
   ],
   "source": [
    "from data.gt_load.datasets import load\n",
    "DATA_PATH = DATA_DOWN_PATH +sep_local+'.gt_datasets'\n",
    "absolute = abspath(DATA_PATH)\n",
    "print(\"DATA_PATH\",absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before np load\n",
      "after np load\n",
      "3\n",
      "4\n",
      "before discrete state space\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = load(dataset_name='dsprites_full', dataset_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gts_csv = os.path.join(recoding_dir, 'csv_dir', 'gts_metrics')\n",
    "gtu_csv = os.path.join(recoding_dir, 'csv_dir', 'gtu_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from training.callbacks.disentangle_supervied import DisentanglementSuperviedMetrics\n",
    "from training.callbacks.disentangle_unsupervied import DisentanglementUnsuperviedMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gts_mertics = DisentanglementSuperviedMetrics(            \n",
    "    ground_truth_data=eval_dataset,\n",
    "    representation_fn=lambda x: ae.encode(inputs={'x_mean': x, 'x_logvar': x})['z_latent'],\n",
    "    random_state=np.random.RandomState(0),\n",
    "    file_Name=gts_csv,\n",
    "    num_train=1000,\n",
    "    num_test=200,\n",
    "    batch_size=batch_size,\n",
    "    continuous_factors=False,\n",
    "    gt_freq=2\n",
    ")\n",
    "gtu_mertics = DisentanglementUnsuperviedMetrics(            \n",
    "    ground_truth_data=eval_dataset,\n",
    "    representation_fn=lambda x: ae.encode(inputs={'x_mean': x, 'x_logvar': x})['z_latent'],\n",
    "    random_state=np.random.RandomState(0),\n",
    "    file_Name=gtu_csv,\n",
    "    num_train=1000,\n",
    "    num_test=200,\n",
    "    batch_size=batch_size,\n",
    "    gt_freq=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f577f4e72005442aa5f899e9cc2aaaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 0', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=50.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n",
      "/home/azeghost/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/dask/array/core.py:1333: FutureWarning: The `numpy.expand_dims` function is not implemented by Dask array. You may want to use the da.map_blocks function or something similar to silence this warning. Your code may stop working in a future release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ae.fit(\n",
    "    x=train_ds,\n",
    "    steps_per_epoch=750,\n",
    "    epochs=100, \n",
    "    verbose=0,\n",
    "    callbacks=[progbar, es, ms, csv_log, sg, gts_mertics, gtu_mertics],\n",
    "    workers=-1,\n",
    "    use_multiprocessing=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
